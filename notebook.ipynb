{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 : Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib import pyplot as plt, image as mpimg\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset from drive to memory in `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
    "\n",
    "feature = breast_cancer_wisconsin_diagnostic.data.features\n",
    "label = breast_cancer_wisconsin_diagnostic.data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 4 copies of the original dataset. For each one, split it into two parts: train and test. The propotion of splitting for each copy is: 60/40, 40/60, 80/20, 90/10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train, label_train = dict(), dict()\n",
    "feature_test, label_test = dict(), dict()\n",
    "\n",
    "train_test_propotions = ['60/40', '40/60', '80/20', '90/10']\n",
    "\n",
    "for propotion in train_test_propotions:\n",
    "    test_size = 1 - int(propotion.split('/')[0]) / 100\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        feature, label, \n",
    "        test_size=test_size,\n",
    "        random_state=42,\n",
    "        stratify=label)\n",
    "    feature_train[propotion] = X_train\n",
    "    label_train[propotion] = y_train\n",
    "    feature_test[propotion] = X_test\n",
    "    label_test[propotion] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train 4 models and save them along with their accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict()\n",
    "\n",
    "for propotion in train_test_propotions:\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(feature_train[propotion], label_train[propotion])\n",
    "    models[propotion] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`export_model_image` function exports visualizing image of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_image(model, filename):\n",
    "    dot_data = export_graphviz(model, \n",
    "                            out_file=None, \n",
    "                            feature_names=[feature_name for feature_name in feature], \n",
    "                            class_names=['benign', 'malignant'],\n",
    "                            filled=True, \n",
    "                            rounded=True,\n",
    "                            special_characters=True)\n",
    "\n",
    "    filename += '.png'\n",
    "    pydotplus.graph_from_dot_data(dot_data).write_png(filename)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    img = mpimg.imread(filename)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export visualizations of all models. Images are saved in `images/decision_tree_classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('images') == False:\n",
    "    os.mkdir('images')\n",
    "\n",
    "if os.path.exists('images/decision_tree_classifier') == False:\n",
    "    os.mkdir('images/decision_tree_classifier')\n",
    "\n",
    "for propotion, model in models.items():\n",
    "    print(f'Visualization of Decision Tree Classifier {propotion}:')\n",
    "    propotion = propotion.replace('/', '-')\n",
    "    export_model_image(model, f'images/decision_tree_classifier/model_on_{propotion}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the decision tree classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `confusion_matrix`\n",
    "\n",
    "In the confusion matrix $C$, the row represents the actual Benign or Malignant classes, while the column represents the prediction of the model.\n",
    "- $C_{0,0}$: Both the truth and the prediction are Benign\n",
    "- $C_{0,1}$: The truth is Benign, but the prediction is Malignant\n",
    "- $C_{1,0}$: The truth is Malignant, but the prediction is Benign\n",
    "- $C_{1,1}$: Both the truth and the prediction are Malignant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `classification_report`\n",
    "\n",
    "The Precision, Recall and F1 Score can be calculated based on which type of class is chosen to be Positive. For example: if Malignant as Positive, Benign as Negative. *For more detail, please refer to the report document.*\n",
    "\n",
    "1. precision: **What percentage of all the Positive predictions made by the model were accurate?** The formula: $$Precision=\\frac{True \\ Positives}{True \\ Positives + False \\ Positives}$$\n",
    "\n",
    "2. recall: **What percentage of all the actual Positives were accurately predicted by the model?** The formula: $$Recall=\\frac{True \\ Positive}{True \\ Positive + False \\ Negative}$$\n",
    "\n",
    "3. F1 Score: **The harmonic mean of Precision and Recall.** If any of them becomes extremely low, F1 Score will also go down. Thus, F1 Score can help you find a good balance between Precision and Recall. The formula: $$F1 \\ Score=\\frac{2 \\times Precision \\times Recall}{Precision + Recall}$$\n",
    "\n",
    "4. Support: **How many samples are in each class.** It uses the ground truth labels, which represent the actual class of each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plot_evaluation(model_name, cls_report, conf_matrix)` function receives model's name, its classification report and confusion matrix to plot evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation(model_name, cls_report, conf_matrix):\n",
    "    print(f'Model: {model_name}')\n",
    "    print(f'Classification Report:\\n{cls_report}')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['benign', 'malignant'])\n",
    "    disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create classification report and confusion matrix for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = dict()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    cls_report = classification_report(label_test[model_name], model.predict(feature_test[model_name])) \n",
    "    conf_matrix = confusion_matrix(label_test[model_name], model.predict(feature_test[model_name]))\n",
    "    evaluation_metrics[model_name] = (cls_report, conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting evaluations of all 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluation('60/40', *evaluation_metrics['60/40'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluation('40/60', *evaluation_metrics['40/60'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluation('80/20', *evaluation_metrics['80/20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluation('90/10', *evaluation_metrics['90/10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For comments, please refer to the report document.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The depth and accuracy of a decision tree\n",
    "\n",
    "This task works on the dataset of 80/20 train-test propotion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propotion = '80/20'\n",
    "\n",
    "X_train, y_train = feature_train[propotion], label_train[propotion]\n",
    "X_test, y_test = feature_test[propotion], label_test[propotion]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 7 models with different maximum depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [None, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "models_max_depth = dict()\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    models_max_depth[max_depth] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export all images of this section inside `images/depth_accuracy`. Save accuracy of each model into `accuracies` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('images/depth_accuracy') == False:\n",
    "    os.mkdir('images/depth_accuracy')\n",
    "\n",
    "accuracies = dict()\n",
    "\n",
    "print(f'Visualization of Decision Tree Classifier on dataset of 80/20')\n",
    "for max_depth, model in models_max_depth.items():\n",
    "    print(f'With maximum depth of {max_depth}:')\n",
    "    export_model_image(model, f'images/depth_accuracy/max_depth_{max_depth}')\n",
    "    accuracies[max_depth] = accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of correlation between values of `max_depth` and `accuracy_score` of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_table = pd.DataFrame({'max_depth': list(accuracies.keys()), 'Accuracy': list(accuracies.values())})\n",
    "accuracy_table = accuracy_table.transpose()\n",
    "accuracy_table.columns = accuracy_table.iloc[0]\n",
    "accuracy_table = accuracy_table[1:]\n",
    "accuracy_table.columns = ['None', '2', '3', '4', '5', '6', '7']\n",
    "\n",
    "accuracy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For comments, please refer to the report document.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
